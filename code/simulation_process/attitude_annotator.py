import asyncio
import sqlite3
import re
import json
import logging
import time
from typing import Dict, List, Optional, Tuple
from openai import AsyncOpenAI

# ... logger å®šä¹‰ä¿æŒä¸å˜ ...
logger = logging.getLogger("AttitudeAnnotation")


# =====================================================================
# 1. åŸºç±» (å·²é‡æž„å¹¶è¡Œé€»è¾‘)
# =====================================================================

# =====================================================================
# 1. åŸºç±» (å·²æ·»åŠ è¶…æ—¶å’Œå¥å£®æ€§)
# =====================================================================

class BaseAttitudeAnnotator:
    """(åŸºç±») æ€åº¦æ ‡æ³¨å™¨çš„å…±äº«é€»è¾‘ã€‚"""
    
    def __init__(
        self, 
        api_key: str, 
        base_url: Optional[str], 
        attitude_columns: List[str],
        concurrency_limit: int = 100, 
        log_interval_posts: int = 100,
        api_timeout_seconds: int = 30  # [æ–°] æ·»åŠ  API è¶…æ—¶
    ):
        """
        åˆå§‹åŒ–åŸºç±»ã€‚

        å‚æ•°:
            ... (å…¶ä»–å‚æ•°) ...
            api_timeout_seconds (int): å•ä¸ª API è¯·æ±‚çš„æœ€å¤§ç­‰å¾…æ—¶é—´ã€‚
        """
        self.api_key = api_key
        self.base_url = base_url
        self.api_timeout_seconds = api_timeout_seconds # [æ–°] å­˜å‚¨è¶…æ—¶
        
        # [ä¿®æ”¹] å°†è¶…æ—¶ä¼ é€’ç»™ OpenAI å®¢æˆ·ç«¯
        self.client = AsyncOpenAI(
            api_key=self.api_key, 
            base_url=self.base_url,
            timeout=self.api_timeout_seconds
        )
        
        self.attitude_columns = attitude_columns
        self.api_semaphore = asyncio.Semaphore(concurrency_limit)
        self.log_interval_posts = max(1, log_interval_posts) 
        
        self.control_chars_regex = re.compile(r'[\x00-\x1F\x7F-\x9F\u200B-\u200D\uFEFF]')
        self.logger = logger
        self.logger.info(
            f"{self.__class__.__name__} initialized. "
            f"Columns: {self.attitude_columns} | "
            f"Concurrency: {concurrency_limit} | "
            f"Log Interval: {self.log_interval_posts} posts | "
            f"API Timeout: {self.api_timeout_seconds}s" # [æ–°] è®°å½•è¶…æ—¶
        )

    # ... _clean_text, _get_system_prompt, _get_attitude_scores_from_llm, 
    #     _setup_database_columns ... 
    #     (è¿™äº›æ–¹æ³•ä¿æŒä¸å˜)
    def _clean_text(self, text) -> str:
        if text is None: return ""
        if isinstance(text, bytes):
            try: text = text.decode('utf-8', errors='replace')
            except Exception: return ""
        if not isinstance(text, str):
            try: text = str(text)
            except Exception: return ""
        cleaned = self.control_chars_regex.sub(' ', text)
        return cleaned.strip()

    def _get_system_prompt(self) -> str:
        return f"""
        You are a content analysis expert. Analyze the user's post.
        
        The post will be provided in one or both of the following forms:
        -   `[User Comment]`: The comment written by the user.
        -   `[Forwarded Original Post]`: The post that the user forwarded or quoted.

        == Your Core Task ==
        Your task is to analyze the sentiment of the **`[User Comment]`**.
        
        1.  **If `[User Comment]` exists:** All your sentiment analysis must be **based on `[User Comment]`**.
            -   `[Forwarded Original Post]` (if present) **is only for context**.
        2.  **If `[User Comment]` does not exist (i.E., the user only forwarded the original post, without commenting):**
            -   In this case, you should analyze the sentiment of the **`[Forwarded Original Post]`**.

        == Scoring and Categorization ==
        1.  **Topic Classification:** Determine the main topic of the analyzed text:
            -   `lifestyle_culture`, `sport_ent`, `sci_health`, `politics_econ`
        2.  **Sentiment Scoring (Continuous Score):** Use a continuous score from -1.0 to 1.0 (float) to precisely assess sentiment intensity.
        3.  **Output Format:**
            -   Assign scores to the matched topics, other topics should be 0.0.
            -   The JSON keys must be: {", ".join(self.attitude_columns)}.
        """

    async def _get_attitude_scores_from_llm(self, content: str) -> Dict[str, float]:
        raise NotImplementedError("Subclass must implement the _get_attitude_scores_from_llm method.")


    def _setup_database_columns(self, conn: sqlite3.Connection, table_name: str):
        self.logger.info(f"Setting up columns for table '{table_name}'...")
        cur = conn.cursor()
        all_columns_to_add = self.attitude_columns + ['attitude_annotated']
        
        for col in all_columns_to_add:
            col_type = "REAL DEFAULT 0.0" if col.startswith("attitude_") else "INTEGER DEFAULT 0"
            if col == "attitude_annotated":
                 col_type = "INTEGER DEFAULT 0"
            try:
                cur.execute(f"ALTER TABLE {table_name} ADD COLUMN {col} {col_type}")
                self.logger.info(f"    - Added column: {col}")
            except sqlite3.OperationalError as e:
                if "duplicate column name" in str(e):
                    self.logger.info(f"    - Column {col} already exists, skipping.")
                else:
                    raise e
        
        self.logger.info(f"Creating annotation index for '{table_name}'...")
        cur.execute(f"CREATE INDEX IF NOT EXISTS idx_{table_name}_annotated ON {table_name} (attitude_annotated);")
        conn.commit()
        cur.close()
        self.logger.info(f"Column setup for '{table_name}' complete.")

    # [å…³é”®ä¿®æ”¹] _process_post å¢žåŠ äº† try...except
    async def _process_post(self, post_id: str, content: str, quote_content: str) -> Optional[Tuple[str, Dict[str, float]]]:
        """ 
        (ç§æœ‰) [ä¿®æ”¹] å¤„ç†å•ä¸ªå¸–å­çš„å®Œæ•´å·¥ä½œæµï¼Œæ·»åŠ å¥å£®çš„å¼‚å¸¸å¤„ç†ã€‚
        """
        try:
            user_comment = self._clean_text(content)
            original_post = self._clean_text(quote_content)
            
            text_to_annotate = ""
            if user_comment:
                text_to_annotate = f"[User Comment]\n{user_comment}"
                if original_post:
                    text_to_annotate += f"\n\n[Forwarded Original Post]\n{original_post}"
            elif original_post:
                text_to_annotate = f"[Forwarded Original Post]\n{original_post}"
            else:
                return None 

            async with self.api_semaphore:
                # [å…³é”®] å¦‚æžœæ­¤è°ƒç”¨è¶…è¿‡ self.api_timeout_seconds,
                # å®ƒå°†å¼•å‘å¼‚å¸¸ï¼Œå¹¶è¢«ä¸‹é¢çš„ except å—æ•èŽ·ã€‚
                scores = await self._get_attitude_scores_from_llm(text_to_annotate)
            
            return (post_id, scores)

        except Exception as e:
            # [å…³é”®] æ•èŽ·æ­¤ä»»åŠ¡ä¸­çš„ä»»ä½•é”™è¯¯ï¼ˆåŒ…æ‹¬ API è¶…æ—¶ï¼‰
            # è®°å½•é”™è¯¯ï¼Œç„¶åŽè¿”å›ž Noneï¼Œè¿™æ ·æ•´ä¸ªæ‰¹å¤„ç†ä¸ä¼šå´©æºƒ
            self.logger.error(f"  -> âŒ Task failed for post_id {post_id}: {e}")
            return None 

    # [é‡æž„] annotate_table (æ­¤æ–¹æ³•ä¿æŒä¸å˜)
    async def annotate_table(self, db_path: str, table_name: str, only_sim_posts: bool = True):
        """
        (å…¬å…±æ–¹æ³•) [é‡æž„] æ ‡æ³¨è¡¨ä¸­æ‰€æœ‰æœªå¤„ç†çš„å¸–å­ã€‚
        ä½¿ç”¨ as_completed æ¥å®žæ—¶ç›‘æŽ§è¿›åº¦ã€‚
        """
        self.logger.info(f"--- ðŸš€ Starting Attitude Annotation for '{table_name}' in {db_path} ---")
        
        conn = None
        all_posts_to_process: List[Tuple] = []
        
        try:
            conn = sqlite3.connect(db_path)
            
            # 1. è®¾ç½®æ•°æ®åº“åˆ— (åŒæ­¥)
            self._setup_database_columns(conn, table_name)
            
            # --- é˜¶æ®µ 1: æ‰¹é‡è¯»å– ---
            self.logger.info("  -> Phase 1: Fetching all posts to process...")
            cur = conn.cursor()
            
            base_query = f"SELECT post_id, content, quote_content FROM {table_name} WHERE attitude_annotated = 0"
            if only_sim_posts:
                self.logger.info("    (Fetching simulator-generated posts only)")
                query_sql = f"{base_query} AND created_at NOT LIKE '%-%'"
            else:
                self.logger.info("    (Fetching ALL posts)")
                query_sql = base_query

            cur.execute(query_sql)
            all_posts_to_process = cur.fetchall()
            cur.close()

            total_to_process = len(all_posts_to_process)
            if total_to_process == 0:
                self.logger.info(f"  -> '{table_name}': No new posts found to annotate.")
                return
            
            self.logger.info(f"  -> Phase 1: Found {total_to_process} posts to annotate.")

            # --- é˜¶æ®µ 2: å…¨é€Ÿå¹¶è¡Œ API (ä½¿ç”¨ as_completed) ---
            self.logger.info(f"  -> Phase 2: Calling LLM API for {total_to_process} posts (Concurrency: {self.api_semaphore._value})...")
            
            tasks = []
            for post_id, content, quote_content in all_posts_to_process:
                tasks.append(
                    self._process_post(post_id, content, quote_content)
                )

            api_start_time = time.time()
            
            update_batch_data = [] # å‡†å¤‡ç”¨äºŽæ•°æ®åº“å†™å…¥
            processed_count = 0
            failed_count = 0 # [æ–°] ç»Ÿè®¡å¤±è´¥æ¬¡æ•°

            # [å…³é”®ä¿®æ”¹] ä½¿ç”¨ as_completed æ›¿æ¢ gather
            for future in asyncio.as_completed(tasks):
                # ç­‰å¾…ä¸‹ä¸€ä¸ªå®Œæˆçš„ä»»åŠ¡
                result = await future
                processed_count += 1
                
                # å¤„ç†ç»“æžœ (å°†å…¶æ·»åŠ åˆ°å¾…å†™å…¥åˆ—è¡¨)
                if result is not None:
                    post_id, scores = result
                    scores_tuple = tuple(scores.get(col, 0.0) for col in self.attitude_columns)
                    update_batch_data.append(scores_tuple + (post_id,))
                else:
                    # [æ–°] å¦‚æžœä»»åŠ¡è¿”å›ž None (å³å¤±è´¥æˆ–è¶…æ—¶)ï¼Œåˆ™è®¡æ•°
                    failed_count += 1
                
                # [è¿›åº¦ç›‘æŽ§]
                # æ¯ N ä¸ªå¸–å­æ‰“å°ä¸€æ¬¡æ—¥å¿—ï¼Œæˆ–è€…åœ¨æœ€åŽä¸€ä¸ªå¸–å­å®Œæˆæ—¶æ‰“å°
                if processed_count % self.log_interval_posts == 0 or processed_count == total_to_process:
                    percent_complete = (processed_count / total_to_process) * 100
                    elapsed_time = time.time() - api_start_time
                    posts_per_sec = processed_count / elapsed_time if elapsed_time > 0 else 0
                    
                    self.logger.info(
                        f"  -> Progress: {processed_count}/{total_to_process} "
                        f"({percent_complete:.1f}%) | "
                        f"Failed: {failed_count} | " # [æ–°] æŠ¥å‘Šå¤±è´¥/è¶…æ—¶æ¬¡æ•°
                        f"Speed: {posts_per_sec:.2f} posts/sec"
                    )

            api_time = time.time() - api_start_time
            self.logger.info(f"  -> Phase 2: LLM processing complete in: {api_time:.2f} seconds. Total Failed/Timeout: {failed_count}")

            # --- é˜¶æ®µ 3: æ‰¹é‡å†™å…¥ ---
            self.logger.info("  -> Phase 3: Writing results to database...")
            
            if not update_batch_data:
                self.logger.info("  -> Phase 3: No valid results to write.")
                return
            
            total_processed = len(update_batch_data)
            write_cur = conn.cursor()
            try:
                set_sql_parts = [f"{col} = ?" for col in self.attitude_columns]
                update_sql = f"UPDATE {table_name} SET {', '.join(set_sql_parts)}, attitude_annotated = 1 WHERE post_id = ?"
                
                write_cur.executemany(update_sql, update_batch_data)
                conn.commit() # [å…³é”®] åª Commit ä¸€æ¬¡ï¼
                self.logger.info(f"  -> Phase 3: Successfully processed and updated {total_processed} posts.")
            except sqlite3.Error as e:
                self.logger.error(f"  -> âŒ Database COMMIT failed: {e}")
                conn.rollback()
            finally:
                write_cur.close()

            self.logger.info(f"--- âœ… '{table_name}' annotation complete ---")

        except Exception as e:
            self.logger.error(f"  -> âŒ Annotation for '{table_name}' failed: {e}")
            import traceback
            traceback.print_exc()
            if conn: conn.rollback()
        finally:
            if conn: conn.close()
# =====================================================================
# 2. vLLM (å¼€æº) å­ç±»
# =====================================================================

# =====================================================================
# 2. vLLM (å¼€æº) å­ç±»
# =====================================================================

class VLLMAttitudeAnnotator(BaseAttitudeAnnotator):
    """(å­ç±») ä½¿ç”¨ vLLM (æœ¬åœ°/å¼€æº) å…¼å®¹ API è¿›è¡Œæ ‡æ³¨ã€‚"""
    
    def __init__(
        self, 
        model_name: str, 
        attitude_columns: List[str],
        base_url: str = "http://localhost:8000/v1",  
        api_key: str = "vllm",  
        concurrency_limit: int = 100,
        log_interval_posts: int = 100,
        api_timeout_seconds: int = 30  # [æ–°]
    ):
        """
        åˆå§‹åŒ– vLLM æ ‡æ³¨å™¨ã€‚
        """
        self.model_name = model_name
        # [ä¿®æ”¹] è°ƒç”¨åŸºç±»çš„ __init__ï¼Œä¼ å…¥æ‰€æœ‰å‚æ•°
        super().__init__(
            api_key=api_key,
            base_url=base_url,
            attitude_columns=attitude_columns,
            concurrency_limit=concurrency_limit,
            log_interval_posts=log_interval_posts,
            api_timeout_seconds=api_timeout_seconds # [æ–°]
        )
        self.logger.info(f"VLLM Annotator using model: {self.model_name}")

    # ... _get_attitude_scores_from_llm æ–¹æ³•ä¿æŒä¸å˜ ...
    async def _get_attitude_scores_from_llm(self, content: str) -> Dict[str, float]:
        # (æ­¤æ–¹æ³•æ— éœ€ä¿®æ”¹)
        default_scores = {col: 0.0 for col in self.attitude_columns}
        if not content or not isinstance(content, str):
            return default_scores
        
        cleaned_content = self._clean_text(content)
        if not cleaned_content:
            return default_scores
        
        system_prompt = self._get_system_prompt()
        system_prompt += "\n\nYou must return **only a single JSON object** and nothing else."
        
        json_text = "" 
        try:
            # (æ­¤è°ƒç”¨çŽ°åœ¨å— self.api_timeout_seconds é™åˆ¶)
            async with self.api_semaphore:
                response = await self.client.chat.completions.create(
                    model=self.model_name, 
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": cleaned_content}
                    ],
                    temperature=0.0
                )
            json_text = response.choices[0].message.content
            
            try:
                start_index = json_text.index("{")
                end_index = json_text.rindex("}")
                json_text = json_text[start_index : end_index + 1]
            except ValueError:
                self.logger.warning(f"  -> Could not find '{{' or '}}' in response. Trying to parse anyway.")
            
            parsed_json = json.loads(json_text)
            validated_scores = {}
            for col in self.attitude_columns:
                value = parsed_json.get(col)
                if not isinstance(value, (int, float)):
                    self.logger.warning(f"  -> Invalid data type for key '{col}'. Got: {value}. Defaulting to 0.0")
                    validated_scores[col] = 0.0
                else:
                    validated_scores[col] = float(value)
            return validated_scores
        except Exception as e:
            # [æ³¨æ„]ï¼šè¿™é‡Œçš„ except å—çŽ°åœ¨ä¸å¤ªå¯èƒ½è¢«è§¦å‘ï¼Œ
            # å› ä¸ºè¶…æ—¶ç­‰é”™è¯¯ä¼šåœ¨ _process_post ä¸­è¢«æ•èŽ·ã€‚
            # ä½†ä¿ç•™å®ƒä»¥é˜² JSON è§£æžç­‰é”™è¯¯ã€‚
            self.logger.error(f"  -> LLM JSON parsing failed: {e}. Raw response: '{json_text}'")
            return default_scores

# =====================================================================
# 3. OpenAI (é—­æº) å­ç±»
# =====================================================================

class OpenAIAttitudeAnnotator(BaseAttitudeAnnotator):
    """(å­ç±») ä½¿ç”¨ OpenAI (é—­æº) å…¼å®¹ API (å¦‚ gpt-4o-mini) è¿›è¡Œæ ‡æ³¨ã€‚"""

    def __init__(
        self, 
        model_name: str, 
        api_key: str,
        attitude_columns: List[str],
        base_url: Optional[str] = None,  
        concurrency_limit: int = 100,
        log_interval_posts: int = 100,
        api_timeout_seconds: int = 30  # [æ–°]
    ):
        """
        åˆå§‹åŒ– OpenAI æ ‡æ³¨å™¨ã€‚
        """
        if not api_key or not api_key.startswith("sk-"):
            raise ValueError("æœ‰æ•ˆçš„ OpenAI API_KEY (sk-...) æœªæä¾›ã€‚")
            
        self.model_name = model_name
        # [ä¿®æ”¹] è°ƒç”¨åŸºç±»çš„ __init__ï¼Œä¼ å…¥æ‰€æœ‰å‚æ•°
        super().__init__(
            api_key=api_key,
            base_url=base_url,
            attitude_columns=attitude_columns,
            concurrency_limit=concurrency_limit,
            log_interval_posts=log_interval_posts,
            api_timeout_seconds=api_timeout_seconds # [æ–°]
        )
        self.logger.info(f"OpenAI Annotator using model: {self.model_name}")

    # ... _get_attitude_scores_from_llm æ–¹æ³•ä¿æŒä¸å˜ ...
    async def _get_attitude_scores_from_llm(self, content: str) -> Dict[str, float]:
        # (æ­¤æ–¹æ³•æ— éœ€ä¿®æ”¹)
        default_scores = {col: 0.0 for col in self.attitude_columns}
        if not content or not isinstance(content, str):
            return default_scores
        
        cleaned_content = self._clean_text(content)
        if not cleaned_content:
            return default_scores
        
        system_prompt = self._get_system_prompt()

        try:
            # (æ­¤è°ƒç”¨çŽ°åœ¨å— self.api_timeout_seconds é™åˆ¶)
            async with self.api_semaphore:
                response = await self.client.chat.completions.create(
                    model=self.model_name, 
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": cleaned_content}
                    ],
                    response_format={"type": "json_object"}, 
                    temperature=0.0
                )
            json_text = response.choices[0].message.content
            
            parsed_json = json.loads(json_text)
            
            validated_scores = {}
            for col in self.attitude_columns:
                value = parsed_json.get(col)
                if not isinstance(value, (int, float)):
                    self.logger.warning(f"  -> Invalid data type for key '{col}'. Got: {value}. Defaulting to 0.0")
                    validated_scores[col] = 0.0
                else:
                    validated_scores[col] = float(value)
            return validated_scores
        except Exception as e:
            self.logger.error(f"  -> LLM call/parse failed: {e}")
            return default_scores